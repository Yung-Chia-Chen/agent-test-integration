from __future__ import annotations
import asyncio
import os
import json
import base64
import re
import requests
from dotenv import load_dotenv
import argparse
import sys
import time
from functools import wraps
from openai import AsyncOpenAI
from litellm import completion
from agents import (
    Agent,
    Model,
    ModelProvider,
    OpenAIChatCompletionsModel,
    RunConfig,
    Runner,
    function_tool,
    set_tracing_disabled,
)
# å…¨åŸŸè®Šæ•¸æ§åˆ¶äº‹ä»¶è¼¸å‡º
_stream_events = False

# Simple in-memory caches to avoid repeated slow network calls during a single
# Python process lifetime. Keys are strings, values are tuples (timestamp, result).
_CACHE: dict = {}
_CACHE_TTL = 300  # seconds

def _get_cached(key: str):
    entry = _CACHE.get(key)
    if not entry:
        return None
    ts, value = entry
    if time.time() - ts > _CACHE_TTL:
        del _CACHE[key]
        return None
    return value

def _set_cache(key: str, value):
    _CACHE[key] = (time.time(), value)

def emit_event(event_type: str, **kwargs):
    """ç™¼é€äº‹ä»¶åˆ°å‰ç«¯ï¼Œåªåœ¨ä¸²æµæ¨¡å¼ä¸‹è¼¸å‡º"""
    if _stream_events:
        event = {
            "type": event_type,
            "timestamp": time.time(),
            **kwargs
        }
        print(json.dumps(event, ensure_ascii=False), flush=True)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®åƒæ•¸ - å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥
BASE_URL = os.getenv("OPENAI_BASE_URL")
API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("OPENAI_MODEL_NAME")

RAGFLOW_BASE_URL = os.getenv("RAGFLOW_BASE_URL")
RAGFLOW_API_KEY = os.getenv("RAGFLOW_API_KEY")
RAGFLOW_KB_ID = os.getenv("RAGFLOW_KB_ID")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")

# æª¢æŸ¥å¿…è¦çš„ç’°å¢ƒè®Šæ•¸
if not all([BASE_URL, API_KEY, MODEL_NAME]):
    raise ValueError("è«‹åœ¨ .env.local ä¸­è¨­ç½® OPENAI_BASE_URL, OPENAI_API_KEY, OPENAI_MODEL_NAME")

if not all([RAGFLOW_BASE_URL, RAGFLOW_API_KEY, RAGFLOW_KB_ID]):
    raise ValueError("è«‹åœ¨ .env.local ä¸­è¨­ç½® RAGFLOW_BASE_URL, RAGFLOW_API_KEY, RAGFLOW_KB_ID")

if not OLLAMA_HOST:
    raise ValueError("è«‹åœ¨ .env.local ä¸­è¨­ç½® OLLAMA_HOST")

# å»ºç«‹è‡ªè¨‚ OpenAI client èˆ‡ provider
client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)
set_tracing_disabled(disabled=True)

class CustomModelProvider(ModelProvider):
    def get_model(self, model_name: str | None) -> Model:
        return OpenAIChatCompletionsModel(model=model_name or MODEL_NAME, openai_client=client)

CUSTOM_MODEL_PROVIDER = CustomModelProvider()

# å‹è™Ÿæ˜ å°„è¡¨
MODEL_MAPPING = {
    "GFM22": "GF-22M",
    "GLM40": "GL-40M",
    "WFM50": "WF-50M",
    "BFM30": "BF-30M",
    "HTFM60": "HT-60F",
    "WE70": "WE-70",
    "QWFM45": "QW-45F"
}

# RAGFlow é…ç½®
RAGFLOW_HEADERS = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {RAGFLOW_API_KEY}'
}

# ============ ç¿»è­¯åŠŸèƒ½ ============

async def detect_language(text: str) -> dict:
    """
    åµæ¸¬æ–‡æœ¬èªè¨€
    
    Args:
        text: è¦åµæ¸¬çš„æ–‡æœ¬
    
    Returns:
        åŒ…å«èªè¨€ä»£ç¢¼å’Œèªè¨€åç¨±çš„å­—å…¸
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{
                "role": "system",
                "content": """ä½ æ˜¯èªè¨€åµæ¸¬å°ˆå®¶ã€‚åµæ¸¬ç”¨æˆ¶è¼¸å…¥çš„èªè¨€ï¼Œä»¥ JSON æ ¼å¼è¿”å›ï¼š
{
  "language_code": "èªè¨€ä»£ç¢¼ï¼ˆå¦‚ en, ja, ko, zh-TW, zh-CN ç­‰ï¼‰",
  "language_name": "èªè¨€åç¨±ï¼ˆå¦‚ English, Japanese, Korean, Traditional Chinese ç­‰ï¼‰",
  "is_chinese": true/false
}

å¸¸è¦‹èªè¨€ä»£ç¢¼ï¼š
- en: English
- ja: Japanese
- ko: Korean
- zh-TW: Traditional Chinese (ç¹é«”ä¸­æ–‡)
- zh-CN: Simplified Chinese (ç°¡é«”ä¸­æ–‡)
- es: Spanish
- fr: French
- de: German
- vi: Vietnamese
- th: Thai"""
            }, {
                "role": "user",
                "content": text
            }],
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        # é è¨­ç‚ºç¹é«”ä¸­æ–‡
        return {
            "language_code": "zh-TW",
            "language_name": "Traditional Chinese",
            "is_chinese": True
        }


async def translate_text(text: str, target_language: str, source_language: str = "zh-TW") -> str:
    """
    ç¿»è­¯æ–‡æœ¬
    
    Args:
        text: è¦ç¿»è­¯çš„æ–‡æœ¬
        target_language: ç›®æ¨™èªè¨€ä»£ç¢¼
        source_language: æºèªè¨€ä»£ç¢¼
    
    Returns:
        ç¿»è­¯å¾Œçš„æ–‡æœ¬
    """
    try:
        # å¦‚æœæºèªè¨€å’Œç›®æ¨™èªè¨€ç›¸åŒï¼Œç›´æ¥è¿”å›
        if source_language == target_language:
            return text
        
        # èªè¨€åç¨±æ˜ å°„
        language_names = {
            "en": "English",
            "ja": "Japanese",
            "ko": "Korean",
            "zh-TW": "Traditional Chinese",
            "zh-CN": "Simplified Chinese",
            "es": "Spanish",
            "fr": "French",
            "de": "German",
            "vi": "Vietnamese",
            "th": "Thai"
        }
        
        target_name = language_names.get(target_language, target_language)
        source_name = language_names.get(source_language, source_language)
        
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{
                "role": "system",
                "content": f"""ä½ æ˜¯å°ˆæ¥­ç¿»è­¯å°ˆå®¶ã€‚å°‡æ–‡æœ¬å¾ {source_name} ç¿»è­¯æˆ {target_name}ã€‚

é‡è¦è¦å‰‡ï¼š
1. ä¿æŒ HTML æ¨™ç±¤ä¸è®Šï¼ˆå¦‚ <table>, <tr>, <td> ç­‰ï¼‰
2. ä¿æŒå°ˆæ¥­è¡“èªçš„æº–ç¢ºæ€§ï¼ˆå¦‚æ¸›é€Ÿæ©Ÿå‹è™Ÿã€æŠ€è¡“è¦æ ¼ï¼‰
3. æ•¸å­—ã€å‹è™Ÿã€å–®ä½ä¿æŒåŸæ¨£
4. è¡¨æ ¼çµæ§‹å®Œå…¨ä¿æŒä¸è®Š
5. ç¿»è­¯è¦è‡ªç„¶ã€æµæš¢ã€å°ˆæ¥­

åªè¿”å›ç¿»è­¯å¾Œçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚"""
            }, {
                "role": "user",
                "content": text
            }],
            temperature=0.3  # é™ä½æº«åº¦ä»¥ç²å¾—æ›´ä¸€è‡´çš„ç¿»è­¯
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        emit_event("error", message=f"ç¿»è­¯å¤±æ•—: {str(e)}")
        return text  # ç¿»è­¯å¤±æ•—å‰‡è¿”å›åŸæ–‡

# ============ ç”¢å“åˆ†æå·¥å…· ============

def extract_type_model(text: str) -> str:
    """å¾OCRçµæœä¸­æå–TYPEå‹è™Ÿ"""
    patterns = [
        r'TYPE[:\s]*([A-Z0-9-]+)',
        r'å‹è™Ÿ[:\s]*([A-Z0-9-]+)',
        r'([A-Z]{2,}[0-9]+[A-Z]*)',
        r'([A-Z]+-[0-9]+[A-Z]*)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).upper()
    
    cleaned_text = text.strip().replace("æœªæ‰¾åˆ°å‹è™Ÿ", "").strip().upper()
    return cleaned_text if cleaned_text else "æœªçŸ¥å‹è™Ÿ"

def map_model_number(model_number: str) -> str:
    """å‹è™Ÿæ˜ å°„è¡¨è½‰æ›"""
    model_upper = model_number.upper()
    return MODEL_MAPPING.get(model_upper, model_number)

@function_tool
async def extract_product_model(image_path: str) -> str:
    """
    å¾ç”¢å“æ¨™ç±¤åœ–ç‰‡ä¸­æå–å‹è™Ÿä¿¡æ¯
    
    Args:
        image_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
    
    Returns:
        è­˜åˆ¥åˆ°çš„ç”¢å“å‹è™Ÿ
    """
    # ç™¼é€å·¥å…·èª¿ç”¨é–‹å§‹äº‹ä»¶
    emit_event("tool_call_start", 
              tool_name="extract_product_model", 
              message="æ­£åœ¨èª¿ç”¨ extract_product_model...")
    
    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            result = f"éŒ¯èª¤ï¼šåœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨ - {image_path}"
            emit_event("tool_call_end", 
                      tool_name="extract_product_model", 
                      message="extract_product_model èª¿ç”¨å®Œæˆ")
            return result
        
        # è½‰æ›åœ–ç‰‡ç‚º base64
        with open(image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        
        image_url = f"data:image/jpeg;base64,{image_base64}"
        
        # OCR æç¤ºè©
        prompt = """è«‹ä»”ç´°è§€å¯Ÿé€™å¼µç”¢å“æ¨™ç±¤åœ–ç‰‡ï¼Œæ‰¾å‡ºTYPEæ¬„ä½çš„å‹è™Ÿè³‡è¨Šã€‚
è«‹åªå›å‚³TYPEå°æ‡‰çš„å‹è™Ÿï¼Œä¾‹å¦‚ï¼šå¦‚æœçœ‹åˆ°TYPE GLM40ï¼Œè«‹å›å‚³ï¼šGLM40
å¦‚æœæ‰¾ä¸åˆ°TYPEæ¬„ä½ï¼Œè«‹å›å‚³ï¼šæœªæ‰¾åˆ°å‹è™Ÿ"""
        
        # èª¿ç”¨ Ollama è¦–è¦ºæ¨¡å‹
        response = completion(
            model="ollama/qwen2.5vl:7b",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": image_url}}
                ]
            }],
            api_base=f"http://{OLLAMA_HOST}",
            stream=False
        )
        
        extracted_text = response.choices[0].message.content
        detected_model = extract_type_model(extracted_text)
        mapped_model = map_model_number(detected_model)
        
        if mapped_model != detected_model:
            result = f"è­˜åˆ¥å‹è™Ÿï¼š{detected_model} â†’ æ˜ å°„å‹è™Ÿï¼š{mapped_model}"
        else:
            result = f"è­˜åˆ¥å‹è™Ÿï¼š{detected_model}"
        
        emit_event("tool_call_end", 
                  tool_name="extract_product_model", 
                  message="extract_product_model èª¿ç”¨å®Œæˆ")
        return result
            
    except Exception as e:
        result = f"åœ–ç‰‡è­˜åˆ¥éŒ¯èª¤ï¼š{str(e)}"
        emit_event("tool_call_error", 
                  tool_name="extract_product_model", 
                  message=f"extract_product_model èª¿ç”¨å¤±æ•—: {str(e)}")
        return result

@function_tool
async def retrieve_product_knowledge(query: str) -> str:
    """
    å¾çŸ¥è­˜åº«æª¢ç´¢ç”¢å“ç›¸é—œä¿¡æ¯
    
    Args:
        query: æœç´¢æŸ¥è©¢å…§å®¹ï¼ˆå¯ä»¥æ˜¯å‹è™Ÿã€ç”¢å“é¡å‹ã€é—œéµè©ç­‰ï¼‰
    
    Returns:
        æª¢ç´¢åˆ°çš„ç”¢å“è³‡æ–™æ‘˜è¦
    """
    emit_event("tool_call_start",
              tool_name="retrieve_product_knowledge",
              message="æ­£åœ¨èª¿ç”¨ retrieve_product_knowledge...")
    start_ts = time.time()

    # Check cache first
    cached = _get_cached(f"retrieve:{query}")
    if cached is not None:
        emit_event("tool_call_end",
                  tool_name="retrieve_product_knowledge",
                  message="retrieve_product_knowledge å¾å¿«å–è¿”å›",
                  cached=True)
        return cached
    
    try:
        # ç›´æ¥ä½¿ç”¨æŸ¥è©¢å…§å®¹æª¢ç´¢
        # Reduce top_k and results to improve latency. Use threadpool to avoid
        # blocking the event loop since requests is synchronous.
        search_data = {
            "question": query,
            "dataset_ids": [RAGFLOW_KB_ID],
            "top_k": 128,
            "similarity_threshold": 0.25,
            "vector_similarity_weight": 0.6,
            "keyword": True,
            "highlight": True
        }

        response = await asyncio.to_thread(
            requests.post,
            f"{RAGFLOW_BASE_URL}/api/v1/retrieval",
            headers=RAGFLOW_HEADERS,
            json=search_data,
            timeout=20,
        )
        
        if response.status_code == 200:
            data = response.json()
            if data and data.get('code') == 0:
                chunks = data.get('data', {}).get('chunks', [])
                
                # åŸºæœ¬éæ¿¾ï¼šåªä¿ç•™ç›¸ä¼¼åº¦è¼ƒé«˜çš„å‰ 5 å€‹çµæœä»¥æ¸›å°‘è™•ç†æ™‚é–“
                filtered_chunks = [
                    chunk for chunk in chunks 
                    if chunk.get('similarity', 0) >= 0.25
                ][:5]
                
                if filtered_chunks:
                    results = []
                    results.append(f"æª¢ç´¢æŸ¥è©¢ï¼š{query}")
                    results.append(f"æ‰¾åˆ° {len(filtered_chunks)} å€‹ç›¸é—œçµæœ\n")
                    
                    for i, chunk in enumerate(filtered_chunks, 1):
                        content = chunk.get('content', '').strip()
                        doc_name = chunk.get('document_keyword', chunk.get('document_name', f'Document{i}'))
                        similarity = chunk.get('similarity', 0)
                        
                        results.append(f"ã€è³‡æ–™ {i}ã€‘")
                        results.append(f"ä¾†æºï¼š{doc_name}")
                        results.append(f"ç›¸ä¼¼åº¦ï¼š{similarity:.3f}")
                        results.append(f"å…§å®¹ï¼š\n{content}")
                        results.append("-" * 50)
                    
                    final_result = "\n".join(results)
                    # Cache the result for short period to speed up repeated queries
                    _set_cache(f"retrieve:{query}", final_result)
                    emit_event("tool_call_end",
                              tool_name="retrieve_product_knowledge",
                              message="retrieve_product_knowledge èª¿ç”¨å®Œæˆ",
                              duration=time.time()-start_ts)
                    return final_result
                else:
                    final_result = f"åœ¨çŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°é—œæ–¼ã€Œ{query}ã€çš„ç›¸é—œè³‡æ–™ã€‚\nå»ºè­°ï¼š\n1. å˜—è©¦ä½¿ç”¨ä¸åŒçš„é—œéµè©\n2. ç¢ºèªå‹è™Ÿæˆ–ç”¢å“åç¨±æ˜¯å¦æ­£ç¢º\n3. æä¾›æ›´å…·é«”çš„ç”¢å“æè¿°"
                    _set_cache(f"retrieve:{query}", final_result)
                    emit_event("tool_call_end",
                              tool_name="retrieve_product_knowledge",
                              message="retrieve_product_knowledge èª¿ç”¨å®Œæˆ",
                              duration=time.time()-start_ts)
                    return final_result
            else:
                final_result = f"çŸ¥è­˜åº«æœç´¢å¤±æ•—ï¼š{data.get('message', 'æœªçŸ¥éŒ¯èª¤')}"
                emit_event("tool_call_end", 
                          tool_name="retrieve_product_knowledge", 
                          message="retrieve_product_knowledge èª¿ç”¨å®Œæˆ")
                return final_result
        else:
            final_result = f"API è«‹æ±‚å¤±æ•—ï¼šHTTP {response.status_code}"
            emit_event("tool_call_end",
                      tool_name="retrieve_product_knowledge",
                      message="retrieve_product_knowledge èª¿ç”¨å®Œæˆ",
                      duration=time.time()-start_ts)
            return final_result
            
    except Exception as e:
        final_result = f"ç”¢å“æœç´¢éŒ¯èª¤ï¼š{str(e)}"
        emit_event("tool_call_error",
                  tool_name="retrieve_product_knowledge",
                  message=f"retrieve_product_knowledge èª¿ç”¨å¤±æ•—: {str(e)}",
                  duration=time.time()-start_ts)
        return final_result

def extract_model_from_query(query: str) -> str:
    """å¾æŸ¥è©¢ä¸­æå–å¯èƒ½çš„å‹è™Ÿ"""
    # å¸¸è¦‹å‹è™Ÿæ¨¡å¼
    patterns = [
        r'\b([A-Z]{1,3}-?\d{1,4}[A-Z]*)\b',  # B-50, GLM40, KOE-155 ç­‰
        r'\b([A-Z]+\d+[A-Z]*)\b',             # GLM40, BFM30 ç­‰
        r'\b([A-Z]-\d+)\b',                   # B-50, F-30 ç­‰
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, query.upper())
        if matches:
            return matches[0]
    
    return None

@function_tool
async def extract_query_keywords(user_query: str) -> str:
    """
    å¾ç”¨æˆ¶æŸ¥è©¢ä¸­æå–é—œéµè©ç”¨æ–¼ RAGFlow æª¢ç´¢
    
    Args:
        user_query: ç”¨æˆ¶çš„æŸ¥è©¢å…§å®¹
    
    Returns:
        æå–çš„é—œéµè©å’Œæœç´¢å»ºè­°
    """
    emit_event("tool_call_start",
              tool_name="extract_query_keywords",
              message="æ­£åœ¨èª¿ç”¨ extract_query_keywords...")

    # Cache check to avoid repeated LLM calls for identical queries
    cached = _get_cached(f"keywords:{user_query}")
    if cached is not None:
        emit_event("tool_call_end",
                  tool_name="extract_query_keywords",
                  message="extract_query_keywords å¾å¿«å–è¿”å›",
                  cached=True)
        return cached

    start_ts = time.time()
    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{
                "role": "system",
                "content": """ä½ æ˜¯æŸ¥è©¢åˆ†æå°ˆå®¶ã€‚å¾ç”¨æˆ¶çš„å•é¡Œä¸­æå–é—œéµè©ç”¨æ–¼æª¢ç´¢æ¸›é€Ÿæ©Ÿè³‡æ–™ã€‚

è«‹åˆ†æç”¨æˆ¶å•é¡Œï¼Œæå–ï¼š
1. å‹è™Ÿï¼ˆå¦‚ B-50, W-70, é›™æ®µè¸è¼ªé½’è¼ªæ¸›é€Ÿæ©Ÿç­‰ï¼‰
2. ç”¢å“é¡å‹ï¼ˆå¦‚ å–®æ®µã€é›™æ®µã€æ³•è˜­å¼ã€ä¸­ç©ºå‹ç­‰ï¼‰
3. æŠ€è¡“è¦æ ¼ï¼ˆå¦‚ é¦¬åŠ›ã€è½‰é€Ÿã€æ‰­çŸ©ç­‰ï¼‰
4. å…¶ä»–é—œéµè©

âš ï¸ ç‰¹åˆ¥æ³¨æ„ã€Œä¸ç¢ºå®šå‹ã€çš„å›ç­”ï¼š
- "ä¸çŸ¥é“"ã€"ä¸æ¸…æ¥š"ã€"æ²’æœ‰"ã€"éš¨ä¾¿"ã€"çœ‹çœ‹"
- "æ²’æœ‰ç‰¹å®šéœ€æ±‚"ã€"é‚„åœ¨è€ƒæ…®"ã€"ä¸ç¢ºå®š"
- é€™äº›å›ç­”è¡¨ç¤ºç”¨æˆ¶éœ€è¦å¼•å°ï¼Œè¨­ç½® needs_overview: true

ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{
    "has_specific_query": true/false,  // æ˜¯å¦æœ‰å…·é«”çš„æŸ¥è©¢å…§å®¹
    "needs_overview": true/false,  // æ˜¯å¦éœ€è¦ç”¢å“æ¦‚è¦½ï¼ˆç•¶ç”¨æˆ¶èªªã€Œä¸çŸ¥é“ã€æ™‚ï¼‰
    "model_numbers": ["å‹è™Ÿ1", "å‹è™Ÿ2"],  // æå–çš„å‹è™Ÿ
    "product_types": ["é¡å‹1", "é¡å‹2"],  // ç”¢å“é¡å‹
    "specifications": ["è¦æ ¼1", "è¦æ ¼2"],  // æŠ€è¡“è¦æ ¼
    "search_query": "æœ€ä½³æœç´¢æŸ¥è©¢",  // ç”¨æ–¼ RAGFlow çš„æœç´¢å­—ä¸²
    "needs_guidance": true/false,  // æ˜¯å¦éœ€è¦å¼•å°ï¼ˆç¬¬ä¸€æ¬¡ä¸æ˜ç¢ºï¼‰
    "user_uncertainty": "high/medium/low"  // ç”¨æˆ¶çš„ä¸ç¢ºå®šç¨‹åº¦
}

åˆ¤æ–·è¦å‰‡ï¼š
- has_specific_query = false + needs_overview = true â†’ ç”¨æˆ¶èªªã€Œä¸çŸ¥é“ã€ï¼Œéœ€è¦å±•ç¤ºç”¢å“æ¦‚è¦½
- has_specific_query = false + needs_guidance = true â†’ é¦–æ¬¡è©¢å•ï¼Œéœ€è¦å¼•å°
- has_specific_query = true â†’ æœ‰æ˜ç¢ºæŸ¥è©¢ï¼Œç›´æ¥æª¢ç´¢"""
            }, {
                "role": "user",
                "content": user_query
            }],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        emit_event("tool_call_end",
                  tool_name="extract_query_keywords",
                  message="é—œéµè©æå–å®Œæˆ",
                  result=result,
                  duration=time.time()-start_ts)

        out = json.dumps(result, ensure_ascii=False)
        _set_cache(f"keywords:{user_query}", out)
        return out

    except Exception as e:
        error_result = {"status": "error", "error": str(e)}
        emit_event("tool_call_error",
                  tool_name="extract_query_keywords",
                  message=f"æå–å¤±æ•—: {str(e)}",
                  duration=time.time()-start_ts)
        return json.dumps(error_result, ensure_ascii=False)

# å»ºç«‹ Agent
async def create_product_analysis_agent():
    """å‰µå»ºç”¢å“åˆ†æ Agent"""
    agent = Agent(
        name="ReducerSelectionAssistant",
        instructions="""You only respond in ç¹é«”ä¸­æ–‡.
ä½ æ˜¯å°ˆæ¥­çš„æ¸›é€Ÿæ©ŸæŸ¥è©¢åŠ©æ‰‹ï¼Œå”åŠ©å®¢æˆ¶æŸ¥è©¢æ¸›é€Ÿæ©Ÿç”¢å“è³‡æ–™ã€‚

## å·¥ä½œæµç¨‹ï¼š

### æ­¥é©Ÿ 1: åˆ†æç”¨æˆ¶æŸ¥è©¢
ä½¿ç”¨ <think> æ¨™ç±¤æ€è€ƒï¼Œç„¶å¾Œèª¿ç”¨ extract_query_keywords åˆ†æç”¨æˆ¶çš„æŸ¥è©¢å…§å®¹ã€‚

### æ­¥é©Ÿ 2: æ ¹æ“šåˆ†æçµæœè™•ç†

**æƒ…å¢ƒ A - æœ‰å…·é«”æŸ¥è©¢å…§å®¹ï¼ˆhas_specific_query: trueï¼‰**
ä¾‹å¦‚ï¼š
- "é›™æ®µè¸è¼ªé½’è¼ªæ¸›é€Ÿæ©Ÿ"
- "å–®æ®µç«‹å¼è¸è¼ªæ¸›é€Ÿæ©Ÿ"
- "è«‹å¹«æˆ‘æŸ¥è©¢ B-50 çš„ç›¸é—œæ•¸æ“š"
- "è«‹å¹«æˆ‘æŸ¥è©¢é›™æ®µè¸è¼ªé½’è¼ªæ¸›é€Ÿæ©Ÿçš„å‹è™Ÿæœ‰å“ªäº›"
- "æˆ‘éœ€è¦æ‰¾ W ç³»åˆ—çš„è¦æ ¼"

è™•ç†æ–¹å¼ï¼š
1. å¾ extract_query_keywords çµæœä¸­ç²å– search_query
2. èª¿ç”¨ retrieve_product_knowledge(search_query) æª¢ç´¢è³‡æ–™
3. å°‡æª¢ç´¢çµæœç”¨æ¸…æ™°çš„æ ¼å¼å‘ˆç¾ï¼ˆè¦æ ¼è¡¨ç”¨ HTML è¡¨æ ¼ï¼‰
4. å¦‚æœæ‰¾åˆ°å¤šå€‹çµæœï¼Œå¹«åŠ©ç”¨æˆ¶ç†è§£å·®ç•°
5. å¦‚æœæ²’æ‰¾åˆ°è³‡æ–™ï¼Œç¦®è²Œå‘ŠçŸ¥ä¸¦å»ºè­°ä½¿ç”¨å…¶ä»–é—œéµè©

**æƒ…å¢ƒ B - ç¬¬ä¸€æ¬¡æŸ¥è©¢ä¸æ˜ç¢ºï¼ˆhas_specific_query: false æˆ– needs_guidance: trueï¼‰**
ä¾‹å¦‚ï¼š
- "æˆ‘éœ€è¦æ¸›é€Ÿæ©Ÿ"
- "ä»€éº¼æ¸›é€Ÿæ©Ÿé©åˆæˆ‘"
- "å¹«æˆ‘æ¨è–¦"

è™•ç†æ–¹å¼ï¼š
è©¢å•ç”¨æˆ¶æ›´å…·é«”çš„éœ€æ±‚ï¼š
```
ç‚ºäº†å¹«æ‚¨æ‰¾åˆ°åˆé©çš„æ¸›é€Ÿæ©Ÿï¼Œè«‹æä¾›ä»¥ä¸‹è³‡è¨Šï¼š

1. æ‚¨æ˜¯å¦æœ‰ç‰¹å®šçš„å‹è™Ÿéœ€æ±‚ï¼Ÿï¼ˆä¾‹å¦‚ï¼šB-50, W-70ï¼‰
2. æˆ–è€…æ‚¨æƒ³äº†è§£å“ªç¨®é¡å‹çš„æ¸›é€Ÿæ©Ÿï¼Ÿï¼ˆä¾‹å¦‚ï¼šå–®æ®µã€é›™æ®µã€æ³•è˜­å¼ã€ä¸­ç©ºå‹ï¼‰
3. ä½¿ç”¨å ´æ™¯ï¼šä¸»è¦ç”¨åœ¨ä»€éº¼æ©Ÿæ¢°ä¸Šï¼Ÿ
4. æŠ€è¡“è¦æ ¼ï¼šé¦¬åŠ›å¤šå°‘ï¼Ÿè¼¸å…¥/è¼¸å‡ºè½‰é€Ÿï¼Ÿ
5. å®‰è£è¦æ±‚ï¼šæœ‰ç‰¹æ®Šçš„å®‰è£æ–¹å¼æˆ–ç©ºé–“é™åˆ¶å—ï¼Ÿ

æä¾›æ›´å¤šè³‡è¨Šå¾Œï¼Œæˆ‘å¯ä»¥ç‚ºæ‚¨æŸ¥è©¢è©³ç´°çš„ç”¢å“è¦æ ¼ã€‚
```

**æƒ…å¢ƒ C - ç”¨æˆ¶å›ç­”ã€Œä¸çŸ¥é“ã€æˆ–ç„¡æ³•æä¾›å…·é«”ä¿¡æ¯**
ä¾‹å¦‚ï¼š
- "ä¸çŸ¥é“"
- "æ²’æœ‰ç‰¹å®šéœ€æ±‚"
- "ä¸æ¸…æ¥š"
- "éš¨ä¾¿çœ‹çœ‹"

âš ï¸ **é‡è¦ï¼šä¸è¦é‡è¤‡è©¢å•ï¼æ”¹ç‚ºä¸»å‹•æä¾›è³‡è¨Šå’Œé¸é …**

è™•ç†æ–¹å¼ï¼š
1. **å…ˆèª¿ç”¨ retrieve_product_knowledge("æ¸›é€Ÿæ©Ÿ")** ç²å–ç”¢å“æ¦‚è¦½
2. **å±•ç¤ºç”¢å“åˆ†é¡å’Œç†±é–€å‹è™Ÿ**ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
æˆ‘äº†è§£æ‚¨é‚„åœ¨æ¢ç´¢éšæ®µï¼Œè®“æˆ‘ç‚ºæ‚¨ä»‹ç´¹æˆ‘å€‘çš„æ¸›é€Ÿæ©Ÿç”¢å“ç³»åˆ—ï¼š

## ğŸ“¦ ä¸»è¦ç”¢å“é¡å‹

æ ¹æ“šçŸ¥è­˜åº«è³‡æ–™ï¼Œæˆ‘å€‘æä¾›ä»¥ä¸‹é¡å‹çš„æ¸›é€Ÿæ©Ÿï¼š

ã€å¾ retrieve_product_knowledge çµæœä¸­æ•´ç†å‡ºç”¢å“åˆ†é¡ã€‘

## ğŸ”¥ ç†±é–€å‹è™Ÿ

ã€å±•ç¤ºæª¢ç´¢åˆ°çš„å¸¸è¦‹å‹è™ŸåŠå…¶åŸºæœ¬ç‰¹é»ã€‘

## ğŸ’¡ é¸æ“‡å»ºè­°

å¦‚æœæ‚¨ï¼š
- éœ€è¦**é«˜æ‰­çŸ©è¼¸å‡º** â†’ å»ºè­°æŸ¥çœ‹é›™æ®µç³»åˆ—
- éœ€è¦**ç¯€çœç©ºé–“** â†’ å»ºè­°æŸ¥çœ‹æ³•è˜­å¼ç³»åˆ—
- éœ€è¦**æ¨™æº–æ‡‰ç”¨** â†’ å»ºè­°æŸ¥çœ‹å–®æ®µç³»åˆ—

æ‚¨å¯ä»¥ï¼š
1. å‘Šè¨´æˆ‘æ‚¨çš„æ‡‰ç”¨å ´æ™¯ï¼ˆå¦‚ï¼šè¼¸é€å¸¶ã€æ”ªæ‹Œæ©Ÿã€å‡é™æ©Ÿç­‰ï¼‰
2. æˆ–ç›´æ¥é¸æ“‡æƒ³äº†è§£çš„å‹è™Ÿæˆ–é¡å‹
3. æˆ–è®“æˆ‘æŸ¥è©¢ã€Œå…¨éƒ¨æ¸›é€Ÿæ©Ÿå‹è™Ÿã€ç‚ºæ‚¨æä¾›å®Œæ•´ç”¢å“ç›®éŒ„

æ‚¨æƒ³äº†è§£å“ªæ–¹é¢å‘¢ï¼Ÿ
```

### æ­¥é©Ÿ 3: å®Œæ•´å›ç­”
- å¿…é ˆçµ¦å‡ºå…·é«”ã€å®Œæ•´çš„å›ç­”
- å¦‚æœæŸ¥è©¢åˆ°è³‡æ–™ï¼Œè¦æ¸…æ¥šå‘ˆç¾ä¸¦è§£é‡‹
- **å¦‚æœç”¨æˆ¶ç„¡æ³•æä¾›æ˜ç¢ºéœ€æ±‚ï¼Œä¸»å‹•å±•ç¤ºç”¢å“åˆ†é¡å’Œé¸é …**
- ä¸è¦åè¦†è©¢å•ç›¸åŒçš„å•é¡Œ
- ä¸è¦ç·¨é€ ä¸å­˜åœ¨çš„è³‡æ–™

### é‡è¦è¦å‰‡ï¼š
1. **éˆæ´»æª¢ç´¢**ï¼šä¸é™å®šç‰¹å®šçš„ç”¢å“é¡å‹ï¼Œç”¨æˆ¶æåˆ°ä»€éº¼å°±æŸ¥ä»€éº¼
2. **å‹è™Ÿå„ªå…ˆ**ï¼šå¦‚æœç”¨æˆ¶æåˆ°å‹è™Ÿï¼ˆB-50, W-70 ç­‰ï¼‰ï¼Œç›´æ¥æŸ¥è©¢è©²å‹è™Ÿ
3. **é¡å‹æŸ¥è©¢**ï¼šå¦‚æœç”¨æˆ¶æåˆ°é¡å‹ï¼ˆé›™æ®µã€æ³•è˜­å¼ç­‰ï¼‰ï¼ŒæŸ¥è©¢è©²é¡å‹çš„ç›¸é—œç”¢å“
4. **ä¸»å‹•å¼•å°**ï¼šç•¶ç”¨æˆ¶ä¸ç¢ºå®šéœ€æ±‚æ™‚ï¼Œå…ˆå±•ç¤ºç”¢å“æ¦‚è¦½ï¼Œå†å¼•å°é¸æ“‡
5. **é¿å…å¾ªç’°**ï¼šå¦‚æœå·²ç¶“è©¢å•éç”¨æˆ¶éœ€æ±‚ï¼Œä¸è¦é‡è¤‡ç›¸åŒå•é¡Œï¼Œæ”¹ç‚ºæä¾›é¸é …
6. **è¦æ ¼å‘ˆç¾**ï¼šç”¢å“è¦æ ¼ç”¨ HTML è¡¨æ ¼æ¸…æ™°å‘ˆç¾
7. **ä¸è¦å‡è¨­**ï¼šä½†ä¹Ÿä¸è¦é™·å…¥åè¦†è©¢å•ï¼Œé©æ™‚ä¸»å‹•æä¾›è³‡è¨Š

### æ ¼å¼è¦å‰‡ï¼š
- ç”¢å“è¦æ ¼ç”¨ HTML è¡¨æ ¼
- ç”¢å“åˆ†é¡ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œåˆ—è¡¨
- å›ç­”è¦å®Œæ•´ã€å‹å–„ã€å°ˆæ¥­
- ä¸è¦è¼¸å‡ºä»£ç¢¼ï¼Œè¦åŸ·è¡Œå·¥å…·ä¸¦è§£é‡‹çµæœ
- ä¸è¦ç·¨é€ è³‡æ–™

ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚""",
        tools=[
            extract_query_keywords,
            extract_product_model,
            retrieve_product_knowledge
        ],
    )
    return agent

async def process_user_input(user_input: str, chat_history: list = None):
    """è™•ç†ç”¨æˆ¶è¼¸å…¥ï¼Œæ•´åˆä¸²æµäº‹ä»¶ã€ç¿»è­¯å’Œå®Œæ•´çµæœ
    
    Args:
        user_input: ç”¨æˆ¶ç•¶å‰è¼¸å…¥
        chat_history: å°è©±æ­·å²ï¼Œæ ¼å¼ç‚º [{"role": "user"/"assistant", "content": "..."}, ...]
    """
    global _stream_events
    _stream_events = True
    
    # å¦‚æœæ²’æœ‰æä¾›æ­·å²è¨˜éŒ„ï¼Œåˆå§‹åŒ–ç‚ºç©ºåˆ—è¡¨
    if chat_history is None:
        chat_history = []
    
    # å„²å­˜åŸå§‹èªè¨€è³‡è¨Š
    original_language = None
    translated_input = user_input
    
    try:
        # ============ æ­¥é©Ÿ 1: èªè¨€åµæ¸¬èˆ‡ç¿»è­¯è¼¸å…¥ ============
        emit_event("language_detection", message="æ­£åœ¨åµæ¸¬èªè¨€...")
        
        language_info = await detect_language(user_input)
        original_language = language_info.get("language_code", "zh-TW")
        language_name = language_info.get("language_name", "Unknown")
        is_chinese = language_info.get("is_chinese", True)
        
        emit_event("language_detected", 
                  language_code=original_language,
                  language_name=language_name,
                  is_chinese=is_chinese,
                  message=f"åµæ¸¬åˆ°èªè¨€: {language_name}")
        
        # å¦‚æœä¸æ˜¯ä¸­æ–‡ï¼Œç¿»è­¯æˆç¹é«”ä¸­æ–‡
        if not is_chinese:
            emit_event("translating_input", 
                      message=f"æ­£åœ¨å°‡ {language_name} ç¿»è­¯æˆç¹é«”ä¸­æ–‡...")
            
            translated_input = await translate_text(
                user_input, 
                target_language="zh-TW",
                source_language=original_language
            )
            
            emit_event("translation_complete",
                      original_text=user_input,
                      translated_text=translated_input,
                      message="è¼¸å…¥ç¿»è­¯å®Œæˆ")
        
        # ============ æ­¥é©Ÿ 2: Agent è™•ç†ï¼ˆåŸæœ‰é‚è¼¯ï¼‰============
        agent = await create_product_analysis_agent()
        
        # æ§‹å»ºåŒ…å«æ­·å²å°è©±çš„å®Œæ•´ context
        # å¦‚æœæœ‰æ­·å²è¨˜éŒ„ï¼Œå°‡å…¶æ ¼å¼åŒ–å¾ŒåŠ å…¥ input
        full_input = translated_input
        if chat_history and len(chat_history) > 0:
            # æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡
            context_parts = ["ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å°è©±è¨˜éŒ„ï¼Œè«‹åƒè€ƒé€™äº›å…§å®¹ä¾†å›ç­”æ–°å•é¡Œï¼š\n"]
            for msg in chat_history[-6:]:  # åªä¿ç•™æœ€è¿‘ 6 æ¢å°è©±ï¼ˆ3è¼ªï¼‰
                role = "ç”¨æˆ¶" if msg.get("role") == "user" else "åŠ©æ‰‹"
                content = msg.get("content", "")
                context_parts.append(f"{role}: {content}")
            
            context_parts.append(f"\nç¾åœ¨çš„æ–°å•é¡Œæ˜¯ï¼š{translated_input}")
            full_input = "\n".join(context_parts)
            
            emit_event("context_added",
                      history_length=len(chat_history),
                      message=f"å·²åŠ å…¥ {len(chat_history)} æ¢æ­·å²å°è©±ä½œç‚ºä¸Šä¸‹æ–‡")
        
        stream_result = Runner.run_streamed(
            agent,
            input=full_input,  # ä½¿ç”¨åŒ…å«æ­·å²çš„å®Œæ•´è¼¸å…¥
            run_config=RunConfig(model_provider=CUSTOM_MODEL_PROVIDER),
            max_turns=10,
        )
        
        thinking_buffer = ""
        inside_think_tag = False
        final_result = None
        all_thinking_content = []
        thinking_sent = False
        
        # ä¸²æµè™•ç†
        async for event in stream_result.stream_events():
            event_type = event.type
            
            try:
                # è™•ç†ä¸åŒé¡å‹çš„äº‹ä»¶
                if event_type == "text_delta":
                    # é€™æ˜¯æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
                    content = ""
                    if hasattr(event, 'delta'):
                        content = str(event.delta)
                    elif hasattr(event, 'data') and hasattr(event.data, 'delta'):
                        content = str(event.data.delta)
                    
                    if content:
                        # æª¢æŸ¥æ˜¯å¦åŒ…å« thinking æ¨™ç±¤
                        if '<think>' in content:
                            inside_think_tag = True
                            thinking_start_idx = content.find('<think>') + 7
                            if thinking_start_idx < len(content):
                                thinking_content = content[thinking_start_idx:]
                                thinking_buffer += thinking_content
                        elif '</think>' in content and inside_think_tag:
                            inside_think_tag = False
                            thinking_end_idx = content.find('</think>')
                            if thinking_end_idx > 0:
                                thinking_content = content[:thinking_end_idx]
                                thinking_buffer += thinking_content
                            
                            # ç™¼é€æ€è€ƒå®Œæˆäº‹ä»¶
                            if thinking_buffer.strip() and not thinking_sent:
                                emit_event("thinking_complete", 
                                          content=thinking_buffer.strip(),
                                          message="æ€è€ƒéç¨‹å®Œæˆ")
                                thinking_sent = True
                            thinking_buffer = ""
                        elif inside_think_tag:
                            # åœ¨æ€è€ƒæ¨™ç±¤å…§çš„å…§å®¹
                            thinking_buffer += content
                
                # è™•ç†å…¶ä»–äº‹ä»¶é¡å‹
                elif event_type in ["raw_response_event", "model_text_delta"]:
                    content = ""
                    if hasattr(event, 'content'):
                        content = str(event.content)
                    elif hasattr(event, 'data'):
                        if hasattr(event.data, 'delta'):
                            content = str(event.data.delta)
                        elif hasattr(event.data, 'content'):
                            content = str(event.data.content)
                        else:
                            content = str(event.data)
                    
                    # åªåœ¨é‚„æ²’ç™¼é€thinkingäº‹ä»¶æ™‚è™•ç†
                    if not thinking_sent and content and not content.startswith('ResponseCreatedEvent'):
                        # æª¢æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„ thinking æ¨™ç±¤
                        if '<think>' in content and '</think>' in content:
                            # æå–å®Œæ•´çš„æ€è€ƒå…§å®¹
                            thinking_start = content.find('<think>') + 7
                            thinking_end = content.find('</think>')
                            if thinking_start < thinking_end:
                                thinking_content = content[thinking_start:thinking_end].strip()
                                if thinking_content:
                                    emit_event("thinking_complete", 
                                              content=thinking_content,
                                              message="æ€è€ƒéç¨‹å®Œæˆ")
                                    thinking_sent = True
                
                
            except Exception as e:
                # éœé»˜è™•ç†éŒ¯èª¤ï¼Œé¿å…ä¸­æ–·æµç¨‹
                continue
            
            # æ•ç²æœ€çµ‚çµæœ
            if event_type == "run_completed":
                try:
                    final_result = event.data if hasattr(event, 'data') else event
                except Exception as e:
                    # éœé»˜è™•ç†éŒ¯èª¤
                    pass
        
        # è™•ç†æœ€çµ‚çµæœ
        if final_result:
            complete_response = final_result.final_output
        else:
            # å¦‚æœæ²’æœ‰æ•ç²åˆ°çµæœï¼Œä½¿ç”¨æ¨™æº–æ–¹å¼ç²å–
            final_result_obj = await Runner.run(
                starting_agent=agent,
                input=translated_input,  # ä½¿ç”¨ç¿»è­¯å¾Œçš„è¼¸å…¥
                run_config=RunConfig(model_provider=CUSTOM_MODEL_PROVIDER),
                max_turns=10,
            )
            complete_response = final_result_obj.final_output
        
        # æå–ä¸¦æ¸…ç†æœ€çµ‚è¼¸å‡º
        final_output = re.sub(r'<think>.*?</think>', '', complete_response, flags=re.DOTALL).strip()
        
        # ============ æ­¥é©Ÿ 3: ç¿»è­¯çµæœå›åŸèªè¨€ ============
        if original_language and original_language != "zh-TW" and not is_chinese:
            emit_event("translating_output",
                      message=f"æ­£åœ¨å°‡çµæœç¿»è­¯å› {language_name}...")
            
            final_output = await translate_text(
                final_output,
                target_language=original_language,
                source_language="zh-TW"
            )
            
            emit_event("translation_complete",
                      message="è¼¸å‡ºç¿»è­¯å®Œæˆ")
        
        # ç™¼é€æœ€çµ‚çµæœ
        emit_event("final_result", 
                  final_output=final_output,
                  user_input=user_input,
                  original_language=original_language,
                  status="success")
        
        return {
            "type": "final_result",
            "final_output": final_output,
            "user_input": user_input,
            "original_language": original_language,
            "status": "success"
        }
        
    except Exception as e:
        emit_event("error", 
                  error=str(e),
                  user_input=user_input,
                  status="error")
        return {
            "type": "error",
            "error": str(e),
            "user_input": user_input,
            "status": "error"
        }
    finally:
        _stream_events = False

async def cli_main():
    """å‘½ä»¤åˆ—ä»‹é¢ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='ç”¢å“åˆ†æ Agent CLI')
    parser.add_argument('--input', required=True, help='ç”¨æˆ¶è¼¸å…¥å…§å®¹')
    parser.add_argument('--history', type=str, default='[]', help='å°è©±æ­·å² (JSON æ ¼å¼)')
    
    args = parser.parse_args()
    
    # è§£ææ­·å²è¨˜éŒ„
    try:
        chat_history = json.loads(args.history) if args.history else []
    except json.JSONDecodeError:
        chat_history = []
        print(json.dumps({
            "type": "warning",
            "message": "ç„¡æ³•è§£æå°è©±æ­·å²ï¼Œå°‡ä½¿ç”¨ç©ºæ­·å²"
        }), flush=True)
    
    try:
        # ä½¿ç”¨çµ±ä¸€çš„è™•ç†å‡½æ•¸
        await process_user_input(args.input, chat_history=chat_history)
        
    except Exception as e:
        error_result = {
            "status": "error", 
            "error": str(e),
            "user_input": args.input
        }
        emit_event("error", 
                  error=str(e),
                  user_input=args.input,
                  status="error")
        sys.exit(1)

# ä¸»ç¨‹å¼é€²å…¥é»
if __name__ == "__main__":
    asyncio.run(cli_main())
